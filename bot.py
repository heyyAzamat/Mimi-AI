import asyncio
import os
from aiogram import Bot, Dispatcher, F
from aiogram.types import Message
from aiogram.filters import CommandStart
from dotenv import load_dotenv

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# --- LOAD ENV ---
load_dotenv()
BOT_TOKEN = os.getenv("BOT_TOKEN")

# --- –¢–µ–ª–µ–≥—Ä–∞–º ---
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–º–∏–Ω–∏-–≤–µ—Ä—Å–∏—è –¥–ª—è CPU/GPU) ---
# –ù–∞–ø—Ä–∏–º–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º MPT-7B-StoryLite (–∏–ª–∏ –ª—é–±—É—é –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å)
MODEL_NAME = "mosaicml/mpt-7b-story-lite"

print("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –ø–∞—Ä—É –º–∏–Ω—É—Ç...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# --- Telegram —Ö—ç–Ω–¥–ª–µ—Ä—ã ---
@dp.message(CommandStart())
async def start(message: Message):
    await message.answer(
        "ü§ñ –ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –ª–æ–∫–∞–ª—å–Ω—ã–π –ò–ò.\n"
        "–ü–∏—à–∏ —á—Ç–æ —É–≥–æ–¥–Ω–æ, –æ—Ç–≤–µ—á—É."
    )

@dp.message(F.text)
async def chat(message: Message):
    user_text = message.text

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    inputs = tokenizer(user_text, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9
    )
    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)

    await message.answer(reply)

# --- –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ---
async def main():
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
